{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical Bikeshare Trip Data\n",
    "\n",
    "\n",
    "In addition to the streaming data from the API, DC Capital Bikeshare provides monthly .csv files in an [S3 Bucket](https://s3.amazonaws.com/capitalbikeshare-data/index.htm). Here's our workflow for this notebook:\n",
    "\n",
    "1. Download the .csv files \n",
    "2. Concatenate them into a master .csv \n",
    "3. Upload this to an S3 bucket of our own \n",
    "4. Query against this table using Athena\n",
    "5. Explore trends in the data.\n",
    "\n",
    "The idea behind aggregating the monthly csvs into a single file is to make querying all the historical data more straightforward, as well as to practice working with the `boto3` S3 SDK. I also published this dataset to [Kaggle](https://www.kaggle.com/datasets/alexsocarras/dc-capital-bikeshare). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import boto3\n",
    "import io\n",
    "import zipfile\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Downloading the CSVs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'capitalbikeshare-data'\n",
    "prefix = ''\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "csv_content = \"\"\n",
    "for obj in response['Contents']:\n",
    "    \n",
    "    if obj['Key'].endswith('.zip'):\n",
    "        \n",
    "        response = s3.get_object(Bucket=bucket_name, Key=obj['Key'])\n",
    "        zipfile_content = zipfile.ZipFile(io.BytesIO(response['Body'].read()))\n",
    "\n",
    "        for filename in zipfile_content.namelist():\n",
    "\n",
    "            if os.path.exists(f\"./data_temp/{filename}\"):\n",
    "                continue\n",
    "\n",
    "            csv_content = zipfile_content.read(filename)\n",
    "         \n",
    "            with open(f\"./data_temp/{filename}\", \"wb\") as fp:\n",
    "                fp.write(csv_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5174216799"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_size = 0\n",
    "for dirpath, dirnames, filenames in os.walk('./data_temp/'):\n",
    "    for f in filenames:\n",
    "        fp = os.path.join(dirpath, f)\n",
    "        total_size += os.path.getsize(fp)\n",
    "\n",
    "total_size # ~4.819 GB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concatenating CSVs\n",
    "\n",
    "Before we concatenate we have to determine the different schemas used in each file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>quarter</th>\n",
       "      <th>header</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2010</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Duration,Start date,End date,Start station num...</td>\n",
       "      <td>./data/2010-capitalbikeshare-tripdata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2011</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Duration,Start date,End date,Start station num...</td>\n",
       "      <td>./data/2011-capitalbikeshare-tripdata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2012</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Duration,Start date,End date,Start station num...</td>\n",
       "      <td>./data/2012Q1-capitalbikeshare-tripdata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>Duration,Start date,End date,Start station num...</td>\n",
       "      <td>./data/2012Q2-capitalbikeshare-tripdata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2012</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>Duration,Start date,End date,Start station num...</td>\n",
       "      <td>./data/2012Q3-capitalbikeshare-tripdata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022</td>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td>ride_id,rideable_type,started_at,ended_at,star...</td>\n",
       "      <td>./data/202210-capitalbikeshare-tripdata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2022</td>\n",
       "      <td>11</td>\n",
       "      <td></td>\n",
       "      <td>ride_id,rideable_type,started_at,ended_at,star...</td>\n",
       "      <td>./data/202211-capitalbikeshare-tripdata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td></td>\n",
       "      <td>ride_id,rideable_type,started_at,ended_at,star...</td>\n",
       "      <td>./data/202212-capitalbikeshare-tripdata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2023</td>\n",
       "      <td>01</td>\n",
       "      <td></td>\n",
       "      <td>ride_id,rideable_type,started_at,ended_at,star...</td>\n",
       "      <td>./data/202301-capitalbikeshare-tripdata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023</td>\n",
       "      <td>02</td>\n",
       "      <td></td>\n",
       "      <td>ride_id,rideable_type,started_at,ended_at,star...</td>\n",
       "      <td>./data/202302-captialbikeshare-tripdata.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    year month quarter                                             header  \\\n",
       "28  2010                Duration,Start date,End date,Start station num...   \n",
       "85  2011                Duration,Start date,End date,Start station num...   \n",
       "10  2012             1  Duration,Start date,End date,Start station num...   \n",
       "0   2012             2  Duration,Start date,End date,Start station num...   \n",
       "87  2012             3  Duration,Start date,End date,Start station num...   \n",
       "..   ...   ...     ...                                                ...   \n",
       "18  2022    10          ride_id,rideable_type,started_at,ended_at,star...   \n",
       "42  2022    11          ride_id,rideable_type,started_at,ended_at,star...   \n",
       "3   2022    12          ride_id,rideable_type,started_at,ended_at,star...   \n",
       "81  2023    01          ride_id,rideable_type,started_at,ended_at,star...   \n",
       "11  2023    02          ride_id,rideable_type,started_at,ended_at,star...   \n",
       "\n",
       "                                       filepath  \n",
       "28    ./data/2010-capitalbikeshare-tripdata.csv  \n",
       "85    ./data/2011-capitalbikeshare-tripdata.csv  \n",
       "10  ./data/2012Q1-capitalbikeshare-tripdata.csv  \n",
       "0   ./data/2012Q2-capitalbikeshare-tripdata.csv  \n",
       "87  ./data/2012Q3-capitalbikeshare-tripdata.csv  \n",
       "..                                          ...  \n",
       "18  ./data/202210-capitalbikeshare-tripdata.csv  \n",
       "42  ./data/202211-capitalbikeshare-tripdata.csv  \n",
       "3   ./data/202212-capitalbikeshare-tripdata.csv  \n",
       "81  ./data/202301-capitalbikeshare-tripdata.csv  \n",
       "11  ./data/202302-captialbikeshare-tripdata.csv  \n",
       "\n",
       "[88 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_metadata = []\n",
    "for file in os.listdir(\"./data_temp\"): \n",
    "\n",
    "    with open(os.path.join(\"./data_temp\", file), \"r\") as fp:\n",
    "        header = fp.readline().strip()\n",
    "        header = re.sub('\"',\"\", header)\n",
    "\n",
    "\n",
    "    year = re.match('[0-9]{4}', file)[0]\n",
    "    try:\n",
    "        quarter = re.search(r'(?<=Q)[0-9]', file)[0]\n",
    "    except: \n",
    "        quarter = \"\" \n",
    "    try: \n",
    "        month = re.search(r'(?<=\\d{4})\\d{2}', file)[0]\n",
    "    except:\n",
    "        month = \"\" \n",
    "\n",
    "    file_metadata.append({'year':year, 'month':month, 'quarter':quarter, 'header':header, 'filepath':'./data_temp/' + file})\n",
    "\n",
    "df = pd.DataFrame(file_metadata)\n",
    "df.sort_values([\"year\",\"month\", \"quarter\"], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Duration,Start date,End date,Start station number,Start station,End station number,End station,Bike number,Member type                                             53\n",
       "ride_id,rideable_type,started_at,ended_at,start_station_name,start_station_id,end_station_name,end_station_id,start_lat,start_lng,end_lat,end_lng,member_casual    35\n",
       "Name: header, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.header.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year                                                     2020\n",
       "month                                                      04\n",
       "quarter                                                      \n",
       "header      ride_id,rideable_type,started_at,ended_at,star...\n",
       "filepath          ./data/202004-capitalbikeshare-tripdata.csv\n",
       "Name: 72, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_header = \"ride_id,rideable_type,started_at,ended_at,start_station_name,start_station_id,end_station_name,end_station_id,start_lat,start_lng,end_lat,end_lng,member_casual\"\n",
    "df[df['header'] == new_header].iloc[0] # New header first used 04/2020"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rideable_type`, `start_lat`, `start_lng`, `end_lat`, and `end_lng` are notable new fields in the new schema format. Let's concatenate the files but keep them separate according to the different schema versions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_schema_files = list(df['filepath'][df['header'] != new_header])\n",
    "new_schema_files = list(df['filepath'][df['header'] == new_header])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "file_gen = (pd.read_csv(file) for file in old_schema_files)\n",
    "\n",
    "df = pd.concat(file_gen, ignore_index=True)\n",
    "df.columns = [re.sub(\" \",\"_\", col.lower()) for col in df.columns]\n",
    "df.to_csv(\"data/bks_tripdata_v1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/portfolio/projects/alaska-etl/venv/lib/python3.7/site-packages/pandas/core/reshape/concat.py:348: DtypeWarning: Columns (5,7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  objs = list(objs)\n"
     ]
    }
   ],
   "source": [
    "file_gen = (pd.read_csv(file) for file in new_schema_files)\n",
    "df = pd.concat(file_gen, ignore_index=True)\n",
    "df.to_csv(\"data/bks_tripdata_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "mv ./data_temp/bks*.csv ../data/\n",
    "\n",
    "rm -rf ./data_temp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload to S3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 = boto3.client('s3')\n",
    "# bucket_name = 'dc-bike-private' # Made this in Management Console\n",
    "# file_path = '../data/bks_tripdata_v2.csv'\n",
    "# object_name = 'dc-bike-private/bks_tripdata_v2.csv'\n",
    "# \n",
    "# with open(file_path, \"rb\") as f:\n",
    "    # s3.upload_fileobj(f, bucket_name, object_name)\n",
    "# \n",
    "# print(f\"File {file_path} uploaded to S3 bucket {bucket_name} with key {object_name}\")\n",
    "\n",
    "## As a function: \n",
    "def upload_file_s3(s3_client, file_path, bucket_name) -> None:\n",
    "\n",
    "  object_name = os.path.basename(file_path)\n",
    "\n",
    "  with open(file_path, \"rb\") as f:\n",
    "    s3_client.upload_fileobj(f, bucket_name, object_name)\n",
    "\n",
    "  print(f\"File {file_path} uploaded to S3 bucket {bucket_name} with key {object_name}\")\n",
    "\n",
    "upload_file_s3(s3, '../data/bks_tripdata_v2.csv', 'dc-bike-private')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Querying the data with Athena \n",
    "\n",
    "After uploading the data to our S3 bucket, I created a database via the Athena console and connected it to our bucket folder `dc-bike-private`. Then I created a table based off the `bks_tripdata_v2.csv` file in our bucket with the following query in the query editor:\n",
    "\n",
    "```sql \n",
    "\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS `dc_bike_private_db`.`bks_tripdata_v2_table` (\n",
    "  `ride_id` string,\n",
    "  `rideable_type` string,\n",
    "  `started_at` timestamp,\n",
    "  `ended_at` timestamp,\n",
    "  `start_station_name` string,\n",
    "  `start_station_id` int,\n",
    "  `end_station_name` string,\n",
    "  `end_station_id` float,\n",
    "  `start_lat` float,\n",
    "  `start_lng` float,\n",
    "  `end_lat` float,\n",
    "  `end_lng` float,\n",
    "  `member_casual` string\n",
    ")\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n",
    "WITH SERDEPROPERTIES ('field.delim' = ',')\n",
    "STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "LOCATION 's3://dc-bike-private/bks_tripdata_v2/'\n",
    "TBLPROPERTIES ('classification' = 'csv');\n",
    "```\n",
    "\n",
    "Note that when you create a table in Athena from data in an S3 bucket, you have to connect to a folder and not directly to a file. Having a folder with multiple files in it can cause serious parsing issues. So you should create a separate folder in your bucket to individually house each file you want to connect.  \n",
    "\n",
    "We can now query against this table using the boto3 SDK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "BUCKET_ID = \"dc-bike-private\"\n",
    "DB_ID = \"dc_bike_private_db\" \n",
    "TABLE_ID = \"bks_tripdata_v2_table\"\n",
    "\n",
    "athena_client = boto3.client('athena')\n",
    "\n",
    "def execute_query(query, database, output_location=f's3://{BUCKET_ID}/query-results/'):\n",
    "    \n",
    "    response = athena_client.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={'Database': database},\n",
    "        ResultConfiguration={'OutputLocation': output_location}\n",
    "    )\n",
    "\n",
    "    query_execution_id = response['QueryExecutionId']\n",
    "    state = None\n",
    "\n",
    "    while state not in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "        response = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "        state = response['QueryExecution']['Status']['State']\n",
    "        time.sleep(1)\n",
    "\n",
    "    if state == 'FAILED':\n",
    "        raise Exception(response['QueryExecution']['Status']['StateChangeReason'])\n",
    "    \n",
    "    result_set = []\n",
    "    column_names = []\n",
    "\n",
    "    results = athena_client.get_query_results(QueryExecutionId=query_execution_id)\n",
    "    for row in results['ResultSet']['Rows']:\n",
    "        if not column_names:\n",
    "            column_names = [col['VarCharValue'] for col in row['Data']]\n",
    "        else:\n",
    "            result_set.append({column_names[i]: row['Data'][i]['VarCharValue'] for i in range(len(column_names))})\n",
    "\n",
    "    return result_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ride_id': 'D3B94F4CE7AC46BC',\n",
       "  'rideable_type': 'docked_bike',\n",
       "  'started_at': '2020-05-25 16:35:25.000',\n",
       "  'ended_at': '2020-05-25 16:48:49.000',\n",
       "  'start_station_name': 'Maine Ave & 7th St SW',\n",
       "  'start_station_id': '31609',\n",
       "  'end_station_name': 'Maine Ave & 9th St SW',\n",
       "  'end_station_id': '31646.0',\n",
       "  'start_lat': '38.878693',\n",
       "  'start_lng': '-77.023056',\n",
       "  'end_lat': '38.88044',\n",
       "  'end_lng': '-77.02524',\n",
       "  'member_casual': 'casual'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"\"\"SELECT * FROM {TABLE_ID} LIMIT 1\"\"\"\n",
    "\n",
    "execute_query(query, DB_ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70623b801652781c2389d9f74154af1ef3dd8a50bfe8b7cd6824c1648ddc5ad0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
