{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import boto3\n",
    "import io\n",
    "import zipfile\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Downloading all files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://s3.amazonaws.com/capitalbikeshare-data/index.html\"\n",
    "# response = requests.get(url)\n",
    "# soup = BeautifulSoup(response.content, \"html.parser\") -- Accessing via s3 instead \n",
    "\n",
    "bucket_name = 'capitalbikeshare-data'\n",
    "prefix = ''\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "csv_content = \"\"\n",
    "for obj in response['Contents']:\n",
    "    \n",
    "    if obj['Key'].endswith('.zip'):\n",
    "        \n",
    "        response = s3.get_object(Bucket=bucket_name, Key=obj['Key'])\n",
    "        zipfile_content = zipfile.ZipFile(io.BytesIO(response['Body'].read()))\n",
    "\n",
    "        for filename in zipfile_content.namelist():\n",
    "\n",
    "            # Check if file exists already\n",
    "            if os.path.exists(f\"./data_temp/{filename}\"):\n",
    "                continue\n",
    "\n",
    "            # Extract the content as bytes object \n",
    "            csv_content = zipfile_content.read(filename)\n",
    "         \n",
    "            # Write to csv \n",
    "            with open(f\"./data_temp/{filename}\", \"wb\") as fp:\n",
    "                fp.write(csv_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5174216799"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "total_size = 0\n",
    "for dirpath, dirnames, filenames in os.walk('./data_temp/'):\n",
    "    for f in filenames:\n",
    "        fp = os.path.join(dirpath, f)\n",
    "        total_size += os.path.getsize(fp)\n",
    "\n",
    "total_size # ~4.819 GB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concatenating Files\n",
    "\n",
    "Before we concatenate we have to determine the different schemas used in each file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>quarter</th>\n",
       "      <th>header</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2010</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Duration,Start date,End date,Start station num...</td>\n",
       "      <td>./data/2010-capitalbikeshare-tripdata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2011</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Duration,Start date,End date,Start station num...</td>\n",
       "      <td>./data/2011-capitalbikeshare-tripdata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2012</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Duration,Start date,End date,Start station num...</td>\n",
       "      <td>./data/2012Q1-capitalbikeshare-tripdata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>Duration,Start date,End date,Start station num...</td>\n",
       "      <td>./data/2012Q2-capitalbikeshare-tripdata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2012</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>Duration,Start date,End date,Start station num...</td>\n",
       "      <td>./data/2012Q3-capitalbikeshare-tripdata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022</td>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td>ride_id,rideable_type,started_at,ended_at,star...</td>\n",
       "      <td>./data/202210-capitalbikeshare-tripdata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2022</td>\n",
       "      <td>11</td>\n",
       "      <td></td>\n",
       "      <td>ride_id,rideable_type,started_at,ended_at,star...</td>\n",
       "      <td>./data/202211-capitalbikeshare-tripdata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td></td>\n",
       "      <td>ride_id,rideable_type,started_at,ended_at,star...</td>\n",
       "      <td>./data/202212-capitalbikeshare-tripdata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2023</td>\n",
       "      <td>01</td>\n",
       "      <td></td>\n",
       "      <td>ride_id,rideable_type,started_at,ended_at,star...</td>\n",
       "      <td>./data/202301-capitalbikeshare-tripdata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023</td>\n",
       "      <td>02</td>\n",
       "      <td></td>\n",
       "      <td>ride_id,rideable_type,started_at,ended_at,star...</td>\n",
       "      <td>./data/202302-captialbikeshare-tripdata.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    year month quarter                                             header  \\\n",
       "28  2010                Duration,Start date,End date,Start station num...   \n",
       "85  2011                Duration,Start date,End date,Start station num...   \n",
       "10  2012             1  Duration,Start date,End date,Start station num...   \n",
       "0   2012             2  Duration,Start date,End date,Start station num...   \n",
       "87  2012             3  Duration,Start date,End date,Start station num...   \n",
       "..   ...   ...     ...                                                ...   \n",
       "18  2022    10          ride_id,rideable_type,started_at,ended_at,star...   \n",
       "42  2022    11          ride_id,rideable_type,started_at,ended_at,star...   \n",
       "3   2022    12          ride_id,rideable_type,started_at,ended_at,star...   \n",
       "81  2023    01          ride_id,rideable_type,started_at,ended_at,star...   \n",
       "11  2023    02          ride_id,rideable_type,started_at,ended_at,star...   \n",
       "\n",
       "                                       filepath  \n",
       "28    ./data/2010-capitalbikeshare-tripdata.csv  \n",
       "85    ./data/2011-capitalbikeshare-tripdata.csv  \n",
       "10  ./data/2012Q1-capitalbikeshare-tripdata.csv  \n",
       "0   ./data/2012Q2-capitalbikeshare-tripdata.csv  \n",
       "87  ./data/2012Q3-capitalbikeshare-tripdata.csv  \n",
       "..                                          ...  \n",
       "18  ./data/202210-capitalbikeshare-tripdata.csv  \n",
       "42  ./data/202211-capitalbikeshare-tripdata.csv  \n",
       "3   ./data/202212-capitalbikeshare-tripdata.csv  \n",
       "81  ./data/202301-capitalbikeshare-tripdata.csv  \n",
       "11  ./data/202302-captialbikeshare-tripdata.csv  \n",
       "\n",
       "[88 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_metadata = []\n",
    "for file in os.listdir(\"./data_temp\"): \n",
    "\n",
    "    with open(os.path.join(\"./data_temp\", file), \"r\") as fp:\n",
    "        header = fp.readline().strip()\n",
    "        header = re.sub('\"',\"\", header)\n",
    "\n",
    "\n",
    "    year = re.match('[0-9]{4}', file)[0]\n",
    "    try:\n",
    "        quarter = re.search(r'(?<=Q)[0-9]', file)[0]\n",
    "    except: \n",
    "        quarter = \"\" \n",
    "    try: \n",
    "        month = re.search(r'(?<=\\d{4})\\d{2}', file)[0]\n",
    "    except:\n",
    "        month = \"\" \n",
    "\n",
    "    file_metadata.append({'year':year, 'month':month, 'quarter':quarter, 'header':header, 'filepath':'./data_temp/' + file})\n",
    "\n",
    "df = pd.DataFrame(file_metadata)\n",
    "df.sort_values([\"year\",\"month\", \"quarter\"], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Duration,Start date,End date,Start station number,Start station,End station number,End station,Bike number,Member type                                             53\n",
       "ride_id,rideable_type,started_at,ended_at,start_station_name,start_station_id,end_station_name,end_station_id,start_lat,start_lng,end_lat,end_lng,member_casual    35\n",
       "Name: header, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.header.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year                                                     2020\n",
       "month                                                      04\n",
       "quarter                                                      \n",
       "header      ride_id,rideable_type,started_at,ended_at,star...\n",
       "filepath          ./data/202004-capitalbikeshare-tripdata.csv\n",
       "Name: 72, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_header = \"ride_id,rideable_type,started_at,ended_at,start_station_name,start_station_id,end_station_name,end_station_id,start_lat,start_lng,end_lat,end_lng,member_casual\"\n",
    "df[df['header'] == new_header].iloc[0] # New header first used 04/2020"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rideable_type`, `start_lat`, `start_lng`, `end_lat`, and `end_lng` are notable new fields in the new schema format. Let's concatenate the files but keep them separate according to the different schema versions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_schema_files = list(df['filepath'][df['header'] != new_header])\n",
    "new_schema_files = list(df['filepath'][df['header'] == new_header])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "file_gen = (pd.read_csv(file) for file in old_schema_files)\n",
    "\n",
    "df = pd.concat(file_gen, ignore_index=True)\n",
    "df.columns = [re.sub(\" \",\"_\", col.lower()) for col in df.columns]\n",
    "df.to_csv(\"data/bks_tripdata_v1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/portfolio/projects/alaska-etl/venv/lib/python3.7/site-packages/pandas/core/reshape/concat.py:348: DtypeWarning: Columns (5,7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  objs = list(objs)\n"
     ]
    }
   ],
   "source": [
    "file_gen = (pd.read_csv(file) for file in new_schema_files)\n",
    "df = pd.concat(file_gen, ignore_index=True)\n",
    "df.to_csv(\"data/bks_tripdata_v2.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move our new files to the main `../data` directory and delete `./data_temp`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "mkdir ../data\n",
    "\n",
    "mv ./data/bks*.csv ../data/\n",
    "\n",
    "rm -rf ./data_temp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70623b801652781c2389d9f74154af1ef3dd8a50bfe8b7cd6824c1648ddc5ad0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
